{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IU3lUvY1K7t"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea4OGyaL1K7v"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zySr16EB1K7v",
        "outputId": "8b80d05f-ae09-405a-eab4-59b060190443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-23 01:09:54--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M  66.7MB/s    in 0.7s    \n",
            "\n",
            "2024-10-23 01:09:55 (66.7 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "replace imdb-reviews-pt-br.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se9ZHKF81K7w"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "chAixOaY1K7w",
        "outputId": "d8a46429-d6bc-4587-9e1b-0e99979451ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5TYSmN1K7x"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SBysqE9U1K7x"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc7dvJSW1K7x"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jy3j_JaT1K7x"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxMYPOGM1K7x"
      },
      "source": [
        "# Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNcMOF3N1K7y"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-o9Tt2gO1K7y"
      },
      "outputs": [],
      "source": [
        "def map1(x):\n",
        "    \"\"\"\n",
        "    Função de mapeamento para extrair filmes negativos.\n",
        "\n",
        "    Parâmetros:\n",
        "    x (Row): Uma linha do DataFrame contendo as colunas.\n",
        "\n",
        "    Retorna:\n",
        "    tuple: Um par chave-valor com o sentimento 'negativo' e o ID.\n",
        "    \"\"\"\n",
        "    sentiment = x['sentiment']\n",
        "    id_ = x['id']\n",
        "\n",
        "    # Apenas mapeia se o sentimento for 'negativo'\n",
        "    if sentiment == 'negativo':\n",
        "        return (1, int(id_))  # Use 1 como chave para facilitar a soma\n",
        "    else:\n",
        "        return None\n",
        "    pass\n",
        "# Aplicando a função Map no DataFrame\n",
        "mapped_rdd = imdb_df.rdd.map(map1).filter(lambda x: x is not None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45SQ4tiG1K7y"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gbqs6svS1K7y"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x, y):\n",
        "    \"\"\"\n",
        "    Função reduce para somar os IDs dos filmes negativos.\n",
        "\n",
        "    Parâmetros:\n",
        "    x (int): O primeiro ID a ser somado.\n",
        "    y (int): O segundo ID a ser somado.\n",
        "\n",
        "    Retorna:\n",
        "    int: A soma dos dois IDs.\n",
        "    \"\"\"\n",
        "    return x + y\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0sEvs2d1K7y"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ROL3IEke1K7y",
        "outputId": "605d26f7-df22-4c7b-a9d8-1b416cacd2e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soma total dos IDs dos filmes negativos: 0\n"
          ]
        }
      ],
      "source": [
        "# Usar reduceByKey para somar todos os IDs de filmes negativos\n",
        "soma_ids = mapped_rdd.reduceByKey(reduceByKey1).map(lambda x: x[1]).sum()\n",
        "\n",
        "# Exibir o resultado\n",
        "print(f\"Soma total dos IDs dos filmes negativos: {soma_ids}\")\n",
        "\n",
        "# Encerrar a sessão Spark\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk_pFVcR1K7y"
      },
      "source": [
        "# Questão 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xiqU1p01K7y"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e uma tupla com a soma das palavras de cada texto como valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH-THwH61K7z"
      },
      "outputs": [],
      "source": [
        "def map2(x):\n",
        "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL9CSSf1K7z"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbLA7KJG1K7z"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0E3JHFM1K7z"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nwQuO491K7z"
      },
      "outputs": [],
      "source": [
        "# Coloque aqui suas linhas de código final\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}